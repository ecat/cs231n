{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkllee/anaconda2/envs/tf/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle as pickle # for saving loss objects\n",
    "\n",
    "import dataset as dd # custom dataset class\n",
    "import models as md\n",
    "\n",
    "# so that when you change an imported file, it changes in the notebook\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_1 = [{'tag': 'pooling_channel_aug_small', 'use_pool': True, 'do_channel_augmentation': True, \n",
    "                 'model_fn': md.get_unet},\n",
    "                {'tag': 'no_pooling_channel_aug_small', 'use_pool': False, 'do_channel_augmentation': True, \n",
    "                 'model_fn': md.get_unet}]\n",
    "\n",
    "model_params_2 = [{'tag': 'pooling_no_channel_aug_small', 'use_pool': True, 'do_channel_augmentation': False, \n",
    "                 'model_fn': md.get_unet},\n",
    "                {'tag': 'no_pooling_no_channel_aug_small', 'use_pool': False, 'do_channel_augmentation': False, \n",
    "                 'model_fn': md.get_unet},\n",
    "                {'tag': 'kaist', 'do_channel_augmentation': False, 'model_fn': md.get_kaist_unet}\n",
    "                ]\n",
    "\n",
    "model_params = model_params_1 + model_params_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tag': 'pooling_channel_aug_small', 'do_channel_augmentation': True, 'use_pool': True, 'model_fn': <function get_unet at 0x7f57fa572cf8>}\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "{'tag': 'no_pooling_channel_aug_small', 'do_channel_augmentation': True, 'use_pool': False, 'model_fn': <function get_unet at 0x7f57fa572cf8>}\n",
      "{'tag': 'pooling_no_channel_aug_small', 'do_channel_augmentation': False, 'use_pool': True, 'model_fn': <function get_unet at 0x7f57fa572cf8>}\n",
      "{'tag': 'no_pooling_no_channel_aug_small', 'do_channel_augmentation': False, 'use_pool': False, 'model_fn': <function get_unet at 0x7f57fa572cf8>}\n",
      "{'tag': 'kaist', 'do_channel_augmentation': False, 'model_fn': <function get_kaist_unet at 0x7f57fa572c80>}\n"
     ]
    }
   ],
   "source": [
    "# cell for going backwards (loading data)\n",
    "from keras.models import load_model\n",
    "\n",
    "# load results\n",
    "results = []\n",
    "\n",
    "for model_param in model_params:\n",
    "    print(model_param)\n",
    "    save_path_model = 'models/' + model_param['tag'] + '.h5'    \n",
    "    save_path_loss_object = 'models/' + model_param['tag'] + '_loss' + '.pkl'\n",
    "    \n",
    "    model = load_model(save_path_model)\n",
    "    loss_dict = pickle.load(open(save_path_loss_object, 'rb'))\n",
    "\n",
    "    results.append((model_param, model, loss_dict))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train loss\n",
    "\n",
    "legend = []\n",
    "for result in results:\n",
    "    \n",
    "    model_param, model, loss_dict = result\n",
    "    \n",
    "    legend = legend + [model_param['tag']]    \n",
    "    plt.plot(np.log10(loss_dict['train_losses_epoch']))   \n",
    "\n",
    "    \n",
    "plt.legend(legend)\n",
    "plt.title('train log loss')\n",
    "plt.ylim([-5, -3])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot test loss\n",
    "\n",
    "for result in results:\n",
    "    \n",
    "    model_param, model, loss_dict = result\n",
    "    plt.plot(np.arange(0, len(loss_dict['test_losses'])) * 9, np.log10(loss_dict['test_losses']))\n",
    "\n",
    "    \n",
    "plt.legend(legend)\n",
    "plt.title('test log loss')\n",
    "plt.ylim([-5, -3])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test images\n",
    "testing_scans = [6]\n",
    "results_with_model_output = []\n",
    "\n",
    "for result in results:\n",
    "    model_param, model, loss_dict = result\n",
    "    print(model_param)\n",
    "    \n",
    "    generator_test = dd.MRImageSequence(scan_numbers=testing_scans, batch_size=10, augment_channels=model_param['do_channel_augmentation'])    \n",
    "    \n",
    "    model_output = model.predict(generator_test.x_transformed[0], batch_size = 10)    \n",
    "    \n",
    "    results_with_model_output = results_with_model_output + [(model_param, model, loss_dict, model_output)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot test images\n",
    "slice_to_show = 120\n",
    "\n",
    "nx, ny, nz, _ = model_output.shape\n",
    "\n",
    "compound_image = np.zeros((ny, nz * len(results)))\n",
    "compound_image_diff = np.zeros((ny, nz * len(results)))\n",
    "order = '| '\n",
    "\n",
    "for idx, result in enumerate(results_with_model_output):\n",
    "    model_param, model, loss_dict, model_output = result\n",
    "    \n",
    "    z_min = idx * nz\n",
    "    z_max = (idx + 1) * nz\n",
    "    \n",
    "    order = order + model_param['tag'] + ' | '\n",
    "    \n",
    "    compound_image[:, z_min:z_max] = np.squeeze(model_output[slice_to_show, :, :])\n",
    "    \n",
    "    diff = 10 * np.abs(np.squeeze(model_output[slice_to_show, :, :]) - np.squeeze(generator_test.y_transformed[0][slice_to_show, :, :]))\n",
    "    compound_image_diff[:, z_min:z_max] = np.squeeze(diff)\n",
    "    \n",
    "fig = plt.figure(figsize=(20, 20))    \n",
    "plt.imshow(compound_image, cmap='gray')\n",
    "plt.title('test predictions \\n ' + order)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "plt.imshow(compound_image_diff, cmap='gray', vmin=0, vmax=1)\n",
    "plt.title('test predictions diff x 10')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D slice viewer from https://www.datacamp.com/community/tutorials/matplotlib-3d-volumetric-data\n",
    "def remove_keymap_conflicts(new_keys_set):\n",
    "    for prop in plt.rcParams:\n",
    "        if prop.startswith('keymap.'):\n",
    "            keys = plt.rcParams[prop]\n",
    "            remove_list = set(keys) & new_keys_set\n",
    "            for key in remove_list:\n",
    "                keys.remove(key)\n",
    "\n",
    "def multi_slice_viewer(volume):\n",
    "    remove_keymap_conflicts({'u', 'i'})\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.volume = volume\n",
    "    ax.index = volume.shape[0] // 2\n",
    "    ax.imshow(volume[ax.index])\n",
    "    fig.canvas.mpl_connect('key_press_event', process_key)\n",
    "\n",
    "def process_key(event):\n",
    "    fig = event.canvas.figure\n",
    "    ax = fig.axes[0]\n",
    "    if event.key == 'u':\n",
    "        previous_slice(ax)\n",
    "    elif event.key == 'i':\n",
    "        next_slice(ax)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "def previous_slice(ax):\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index - 1) % volume.shape[0]  # wrap around using %\n",
    "    ax.images[0].set_array(volume[ax.index])\n",
    "\n",
    "def next_slice(ax):\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index + 1) % volume.shape[0]\n",
    "    ax.images[0].set_array(volume[ax.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_volume = np.zeros((nx, ny, nz * len(results)))\n",
    "\n",
    "for idx, result in enumerate(results_with_model_output):\n",
    "    model_param, model, loss_dict, model_output = result\n",
    "    \n",
    "    z_min = idx * nz\n",
    "    z_max = (idx + 1) * nz\n",
    "    \n",
    "    order = order + model_param['tag'] + ' | '\n",
    "    \n",
    "    compound_volume[:, :, z_min:z_max] = np.squeeze(model_output[:, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multi_slice_viewer(compound_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
